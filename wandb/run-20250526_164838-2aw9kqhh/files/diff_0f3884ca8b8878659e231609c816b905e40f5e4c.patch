diff --git a/gym_pybullet_drones/examples/learn_multi.py b/gym_pybullet_drones/examples/learn_multi.py
index 0ab16d8..7443fd4 100644
--- a/gym_pybullet_drones/examples/learn_multi.py
+++ b/gym_pybullet_drones/examples/learn_multi.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 """
 train_multi_improved.py: Train a multi-drone swarm with improved target-following behavior
-Enhanced with comprehensive WandB logging
+Enhanced with comprehensive WandB logging and model weight pushing after evaluations
 """
 import os
 import time
@@ -109,13 +109,30 @@ class EnhancedWandbCallback(BaseCallback):
 
 
 class DetailedEvalCallback(EvalCallback):
-    """Enhanced evaluation callback with detailed logging"""
+    """Enhanced evaluation callback with detailed logging and model weight pushing"""
     
+    def __init__(self, eval_env, save_freq_evals=1, push_to_wandb=True, **kwargs):
+        """
+        Parameters:
+        - save_freq_evals: Save and push model weights every N evaluations (default: 1 = every evaluation)
+        - push_to_wandb: Whether to push model weights to wandb as artifacts
+        """
+        super().__init__(eval_env, **kwargs)
+        self.save_freq_evals = save_freq_evals
+        self.push_to_wandb = push_to_wandb
+        self.eval_count = 0
+        self.model_save_path = kwargs.get('best_model_save_path', './models')
+        
+        # Ensure model save directory exists
+        os.makedirs(self.model_save_path, exist_ok=True)
+        
     def _on_step(self) -> bool:
         result = super()._on_step()
         
-        # Log additional evaluation metrics when evaluation occurs
+        # Check if evaluation occurred
         if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
+            self.eval_count += 1
+            
             # Get the latest evaluation results
             if len(self.evaluations_results) > 0:
                 latest_results = self.evaluations_results[-1]
@@ -124,44 +141,111 @@ class DetailedEvalCallback(EvalCallback):
                 min_reward = np.min(latest_results)
                 max_reward = np.max(latest_results)
                 
+                # Log evaluation metrics
                 wandb.log({
                     'eval/mean_reward': mean_reward,
                     'eval/std_reward': std_reward,
                     'eval/min_reward': min_reward,
                     'eval/max_reward': max_reward,
                     'eval/num_episodes': len(latest_results),
+                    'eval/evaluation_count': self.eval_count,
                 }, step=self.num_timesteps)
                 
                 print(f"[EVAL] Step {self.num_timesteps}: Mean={mean_reward:.2f}Â±{std_reward:.2f}, Range=[{min_reward:.2f}, {max_reward:.2f}]")
+                
+                # Save and push model weights at specified intervals
+                if self.eval_count % self.save_freq_evals == 0:
+                    self._save_and_push_model(mean_reward)
         
         return result
+    
+    def _save_and_push_model(self, mean_reward):
+        """Save model weights and push to wandb"""
+        try:
+            # Create timestamped model filename
+            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
+            model_filename = f"model_eval_{self.eval_count}_step_{self.num_timesteps}_reward_{mean_reward:.2f}_{timestamp}.zip"
+            model_path = os.path.join(self.model_save_path, model_filename)
+            
+            # Save the model
+            self.model.save(model_path)
+            print(f"[MODEL SAVE] Saved model to: {model_path}")
+            
+            # Push to wandb as artifact if enabled
+            if self.push_to_wandb:
+                self._push_to_wandb(model_path, model_filename, mean_reward)
+                
+        except Exception as e:
+            print(f"[ERROR] Failed to save/push model: {e}")
+    
+    def _push_to_wandb(self, model_path, model_filename, mean_reward):
+        """Push model to wandb as artifact"""
+        try:
+            # Create artifact with metadata
+            artifact_name = f"model_eval_{self.eval_count}"
+            artifact = wandb.Artifact(
+                artifact_name, 
+                type='model',
+                metadata={
+                    'eval_count': self.eval_count,
+                    'timestep': self.num_timesteps,
+                    'mean_reward': mean_reward,
+                    'model_type': 'PPO',
+                    'eval_timestamp': datetime.now().isoformat()
+                }
+            )
+            
+            # Add model file
+            artifact.add_file(model_path, name=model_filename)
+            
+            # Log artifact to wandb
+            wandb.log_artifact(artifact, aliases=[f"eval_{self.eval_count}", "latest_eval"])
+            
+            print(f"[WANDB] Pushed model artifact: {artifact_name}")
+            
+            # Log model push event
+            wandb.log({
+                'model_push/eval_count': self.eval_count,
+                'model_push/timestep': self.num_timesteps,
+                'model_push/mean_reward': mean_reward,
+                'model_push/artifact_name': artifact_name,
+            }, step=self.num_timesteps)
+            
+        except Exception as e:
+            print(f"[ERROR] Failed to push model to wandb: {e}")
 
 
-def create_target_sequence(num_drones=4, scale=1.5):
+def create_target_sequence(num_drones=4, scale=0.5):
     """Create a challenging but achievable target sequence"""
     
     if num_drones == 4:
         # 4-drone formations
         targets = np.array([
             # Phase 0: Square formation
-            [[ scale,  scale, 1.5], [-scale,  scale, 1.5], 
-             [-scale, -scale, 1.5], [ scale, -scale, 1.5]],
+            [[ scale,  scale, 0.5], [-scale,  scale, 0.5], 
+             [-scale, -scale, 0.5], [ scale, -scale, 0.5]],
+            [[ scale,  scale, 0.5], [-scale,  scale, 0.5], 
+             [-scale, -scale, 0.5], [ scale, -scale, 0.5]],
+            [[ scale,  scale, 0.5], [-scale,  scale, 0.5], 
+             [-scale, -scale, 0.5], [ scale, -scale, 0.5]],
+            [[ scale,  scale, 0.5], [-scale,  scale, 0.5], 
+             [-scale, -scale, 0.5], [ scale, -scale, 0.5]],
             
-            # Phase 1: Rotate clockwise
-            [[-scale,  scale, 1.5], [-scale, -scale, 1.5], 
-             [ scale, -scale, 1.5], [ scale,  scale, 1.5]],
+            # # Phase 1: Rotate clockwise
+            # [[-scale,  scale, 0.5], [-scale, -scale, 0.5], 
+            #  [ scale, -scale, 0.5], [ scale,  scale, 0.5]],
             
-            # Phase 2: Diamond formation (higher altitude)
-            [[ 0.0,  scale*1.2, 2.0], [-scale*1.2,  0.0, 2.0], 
-             [ 0.0, -scale*1.2, 2.0], [ scale*1.2,  0.0, 2.0]],
+            # # Phase 2: Diamond formation (higher altitude)
+            # [[ 0.0,  scale*1.2, 2.0], [-scale*1.2,  0.0, 2.0], 
+            #  [ 0.0, -scale*1.2, 2.0], [ scale*1.2,  0.0, 2.0]],
             
-            # Phase 3: Tight formation at center
-            [[ 0.3,  0.3, 1.8], [-0.3,  0.3, 1.8], 
-             [-0.3, -0.3, 1.8], [ 0.3, -0.3, 1.8]],
+            # # Phase 3: Tight formation at center
+            # [[ 0.3,  0.3, 1.8], [-0.3,  0.3, 1.8], 
+            #  [-0.3, -0.3, 1.8], [ 0.3, -0.3, 1.8]],
              
-            # Phase 4: Line formation
-            [[ 0.0,  scale, 1.5], [ 0.0,  scale/3, 1.5], 
-             [ 0.0, -scale/3, 1.5], [ 0.0, -scale, 1.5]]
+            # # Phase 4: Line formation
+            # [[ 0.0,  scale, 0.5], [ 0.0,  scale/3, 0.5], 
+            #  [ 0.0, -scale/3, 0.5], [ 0.0, -scale, 0.5]]
         ])
     else:
         # For other numbers of drones, create circular formations
@@ -170,7 +254,7 @@ def create_target_sequence(num_drones=4, scale=1.5):
         for phase in range(n_phases):
             phase_targets = []
             radius = scale * (1.0 + 0.2 * phase)  # Varying radius
-            height = 1.5 + 0.3 * phase  # Varying height
+            height = 0.5 + 0.3 * phase  # Varying height
             for i in range(num_drones):
                 angle = 2 * np.pi * i / num_drones + phase * np.pi / 4  # Rotate each phase
                 x = radius * np.cos(angle)
@@ -182,7 +266,7 @@ def create_target_sequence(num_drones=4, scale=1.5):
     return targets.astype(np.float32)
 
 
-def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_entity):
+def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_entity, save_freq_evals):
     # Initialize Weights & Biases with comprehensive config
     run_name = f"multi_target_swarm_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
     
@@ -201,8 +285,9 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
         'n_steps': 2048,
         'clip_range': 0.2,
         'ent_coef': 0.01,
-        'eval_freq': 10000,
+        'eval_freq': 100000,
         'log_freq': 1000,
+        'save_freq_evals': save_freq_evals,  # New parameter
     }
     
     # Initialize wandb with more detailed settings
@@ -232,6 +317,7 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
     print(f"[INFO] Created target sequence with shape: {target_sequence.shape}")
     print(f"[INFO] Steps per target: {steps_per_target}")
     print(f"[INFO] Total episode length: {len(target_sequence) * steps_per_target} steps")
+    print(f"[INFO] Model weights will be saved every {save_freq_evals} evaluation(s)")
 
     # Vectorized training environment
     def make_env():
@@ -246,7 +332,7 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
         )
         return Monitor(env)
     
-    train_env = make_vec_env(make_env, n_envs=1, seed=0)
+    train_env = make_vec_env(make_env, n_envs=4, seed=0)
     
     # Evaluation environment
     eval_env = MultiTargetAviary(
@@ -282,7 +368,7 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
     # Log model architecture and watch gradients
     wandb.watch(model.policy, log='all', log_freq=1000, log_graph=True)
 
-    # Enhanced callbacks with comprehensive logging
+    # Enhanced callbacks with comprehensive logging and model pushing
     enhanced_wandb_cb = EnhancedWandbCallback(
         log_freq=config['log_freq'],
         verbose=1
@@ -296,7 +382,9 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
         n_eval_episodes=5,
         deterministic=True,
         render=False,
-        verbose=1
+        verbose=1,
+        save_freq_evals=save_freq_evals,  # New parameter
+        push_to_wandb=True
     )
     
     # Standard WandB callback for additional SB3 metrics
@@ -316,6 +404,7 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
         'setup/phases': len(target_sequence),
         'setup/steps_per_phase': steps_per_target,
         'setup/control_frequency': freq,
+        'setup/save_freq_evals': save_freq_evals,
     })
 
     # Train the model
@@ -336,10 +425,15 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
     final_path = os.path.join(save_dir, 'final_model.zip')
     model.save(final_path)
     
-    # Upload model as wandb artifact
-    artifact = wandb.Artifact('model', type='model')
-    artifact.add_file(final_path)
-    wandb.log_artifact(artifact)
+    # Upload final model as wandb artifact
+    final_artifact = wandb.Artifact('final_model', type='model', metadata={
+        'training_complete': True,
+        'total_timesteps': config['total_timesteps'],
+        'training_time_seconds': training_time,
+        'final_model': True
+    })
+    final_artifact.add_file(final_path)
+    wandb.log_artifact(final_artifact, aliases=["final", "best"])
 
     # Final evaluation with detailed logging
     print("[INFO] Running final evaluation...")
@@ -363,6 +457,7 @@ def run(output_folder, gui, record_video, plot, local, wandb_project, wandb_enti
     summary_table.add_data("Training Time (min)", f"{training_time/60:.2f}")
     summary_table.add_data("Total Timesteps", f"{config['total_timesteps']:,}")  # Format as string with commas
     summary_table.add_data("Number of Drones", f"{DEFAULT_DRONES}")  # Convert to string
+    summary_table.add_data("Model Save Frequency", f"Every {save_freq_evals} evaluation(s)")
     wandb.log({"training_summary": summary_table})
 
     # Demonstration
@@ -499,6 +594,8 @@ if __name__ == '__main__':
                         help='Weights & Biases project name')
     parser.add_argument('--wandb_entity', default=None, type=str, 
                         help='Weights & Biases entity/username')
+    parser.add_argument('--save_freq_evals', default=1, type=int,
+                        help='Save and push model weights every N evaluations (default: 1)')
     
     args = parser.parse_args()
 
@@ -509,5 +606,6 @@ if __name__ == '__main__':
         args.plot,
         args.local,
         args.wandb_project,
-        args.wandb_entity
+        args.wandb_entity,
+        args.save_freq_evals
     )
\ No newline at end of file
